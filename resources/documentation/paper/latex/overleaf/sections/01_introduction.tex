\section{Introduction}

Large language models have enabled AI systems that reason, plan, and act across complex multi-step workflows. Frameworks like LangChain, AutoGPT, and AutoGen demonstrate how agents can decompose problems and coordinate actions through sequences of LLM calls. However, as these workflows grow more ambitious---chaining dozens of outputs, invoking tools, branching on intermediate results---a critical failure mode emerges: \textbf{context pollution}.

\subsection{The Problem: Context Pollution in Multi-Step Reasoning}

Context pollution occurs when accumulated information across reasoning steps causes models to hallucinate, confuse intermediate outputs with inputs, and lose track of original task constraints. Empirical studies show that LLMs ``forget'' earlier facts in long conversations, leading to contradictory decisions on lengthy tasks~\cite{chang2025, saga2025}. The compounding error effect is severe: small mistakes in early steps propagate through sequential reasoning, making complex multi-step solutions exponentially more error-prone than simple ones~\cite{wand2025}.

Even state-of-the-art models like GPT-4 struggle to maintain global consistency on complicated plans, often abandoning earlier constraints as context grows. Simply expanding context windows does not solve the problem---research shows that very long prompts can confuse models or cause them to focus on irrelevant details, actually \textit{reducing} correctness despite larger windows~\cite{breunig2025}.

Current approaches offer limited defense. Direct prompting bundles everything into one context window, creating cognitive overload. Chain-of-Thought (CoT) prompting extends interaction step-by-step but does not isolate context between steps---hallucinated thoughts in early reasoning leak forward into later prompts. Agent frameworks like LangChain and AutoGPT provide orchestration but leave data flow implicit~\cite{ibm2025, wang2024}: when step 7 of a 10-step chain fails, developers cannot easily determine what step 7 actually saw. As practitioners note, debugging such agents often happens ``in the dark,'' requiring reverse-engineering of hidden state to find root causes~\cite{patel2025}.

\subsection{The Solution: Explicit Data Isolation by Construction}

NormCode takes a fundamentally different approach: it enforces \textbf{explicit data isolation as a language-level constraint}. Rather than letting context bleed implicitly between steps, NormCode defines \textit{plans of inferences}---structured decompositions where each step is a self-contained logical unit with access only to explicitly passed inputs. If an early step processes a raw document and later steps receive only a summarized excerpt, no subsequent inference can accidentally peek at the original---it simply is not in that step's input by design.

This idea aligns with emerging strategies in advanced agent design. Experts advocate isolating context into separate threads or sub-agents, each handling narrow subtasks with only relevant data, rather than one monolithic agent juggling a combined context~\cite{ruan2024}. Recent work on execution isolation (e.g., IsolateGPT) proposes sandboxing LLM-based applications to prevent unauthorized data access~\cite{wu2025}. NormCode builds this isolation into the language itself---not as a guideline but as an enforced rule of the programming model.

The result is full auditability. Every intermediate state is explicit and inspectable, providing process transparency into how conclusions are reached. For high-stakes domains---legal analysis, medical reasoning, financial decision-making---this transparency is not optional~\cite{sai2025}. Regulators demand that AI systems provide decision traceability and justification (e.g., EU AI Act requirements for high-impact systems). NormCode produces an audit trail of reasoning that can be verified step-by-step, dramatically improving trust.

\subsection{Contributions}

This paper presents NormCode, a semi-formal language and execution framework for context-isolated AI planning. Our contributions are as follows.

First, we introduce \textbf{a novel intermediate representation} for AI planning based on inference decomposition and explicit data flow, bridging natural language intent and machine execution while maintaining auditability. The representation uses tensor-structured references with named axes to organize data flow precisely.

Second, we establish \textbf{semantic/syntactic separation} as a clean architectural distinction between LLM-driven operations (expensive, non-deterministic) and data-restructuring operations (free, deterministic), enabling precise cost and reliability tracing.

Third, we provide \textbf{a multi-format ecosystem} (\texttt{.ncds} for draft authoring, \texttt{.ncd} for formal execution, \texttt{.ncn} for natural language verification, \texttt{.ncdn} for hybrid editing) supporting human authoring, machine rigor, and human verification within a single coherent pipeline---enabling progressive formalization from exploratory sketch to production system.

Fourth, we describe \textbf{a four-phase compilation pipeline} (Derivation, Formalization, Post-Formalization, Activation) that transforms natural language intent into executable JSON repositories through progressive refinement.

Fifth, we present \textbf{a working implementation} including an orchestrator with dependency-driven scheduling, SQLite-backed checkpointing, and loop management, as well as a visual Canvas App debugger built on React Flow and FastAPI providing real-time graph visualization, breakpoint debugging, and multi-agent configuration.

Sixth, we validate the approach through \textbf{two demonstrations}: (a) 100\% accuracy on base-X addition tasks across arbitrary digit lengths, and (b) self-hosted execution of NormCode's own compiler pipeline, demonstrating both correctness and expressive completeness.

The rest of this paper is organized as follows: Section 2 reviews related work in AI planning, agent frameworks, and intermediate representations. Sections 3-4 describe the NormCode language and reference system. Section 5 articulates the design philosophy. Sections 6-7 detail the execution model and compiler ecosystem. Section 8 presents case study evaluations. Section 9 discusses limitations and future work, and Section 10 concludes.

