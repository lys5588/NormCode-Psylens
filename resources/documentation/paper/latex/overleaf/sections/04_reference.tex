\section{The Reference System}

While concepts define the \textit{meaning} of data, the \textbf{Reference System} provides the machinery for storing and manipulating it. In NormCode, every piece of data---from a single file path to a collection of user queries---is encapsulated in a \textbf{Reference}, a structure inspired by multi-dimensional tensors.

\subsection{References as Tensors with Named Axes}

Unlike standard lists or dictionaries, a Reference organizes data along named axes (e.g., \texttt{['student', 'assignment']} rather than \texttt{[0, 1]}). This allows operations to be robust to changes in data shape. A collection of documents is not just a list; it is a tensor with a \texttt{document} axis. If we process each document to extract three features, the result is automatically a 2D tensor with \texttt{['document', 'feature']} axes. This explicit structure prevents the ``shape mismatch'' errors common in ad-hoc prompt chains.

\subsection{Perceptual Signs: The ``Lazy Evaluation'' of AI}

Carrying large data payloads (e.g., full text of books) through every step of a plan is inefficient and creates context pollution. NormCode solves this with \textbf{Perceptual Signs}: lightweight pointers that represent data without loading it.

A sign follows the format \texttt{\%\{Norm\}ID(Signifier)}. For example:
\begin{verbatim}
%{file_location}a1b(data/contract.pdf)
\end{verbatim}

This tells the agent: ``There is an object here (ID \texttt{a1b}). To perceive it, use your \texttt{FileSystem} faculty (the \texttt{file\_location} norm) on the path \texttt{data/contract.pdf}.'' The actual data is only loaded (``transmuted'') at the exact moment an inference needs to operate on it (the \textbf{MVP} step). Until then, the system passes around the lightweight sign, ensuring efficiency.

\subsection{Semantic vs. Syntactic Operations on References}

The distinction between semantic and syntactic operations is implemented at the Reference level through a layered algebra.

\textbf{1. Syntactic Operations (Data Plumbing)}: Operations like \textbf{Reshaping} (\texttt{slice}, \texttt{append}) and \textbf{Combining} (\texttt{cross\_product}, \texttt{join}) manipulate the \textit{structure} of the tensor without looking at the content. They are instant, deterministic, and cost zero tokens.

\textbf{2. Semantic Operations (AI Reasoning)}: The \textbf{Cross-Action} (\texttt{cross\_action(functions\_ref, values\_ref)}) is the bridge between structure and meaning, applying an LLM-prepared function to values element-by-element.

By restricting LLMs to \texttt{cross\_action} and handling all other data movement via syntactic operators, NormCode ensures that AI reasoning is applied only where strictly necessary.

